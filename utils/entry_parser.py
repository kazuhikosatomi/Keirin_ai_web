import re
import os
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

def extract_lineinfo_from_html(html):
    soup = BeautifulSoup(html, "html.parser")
    line_groups = []
    current_group = []

    square_tags = soup.select("div.g-flex span")
    for tag in square_tags:
        if "square" in tag.get("class", []):
            try:
                car_number = int(tag.get_text(strip=True))
                current_group.append(car_number)
            except ValueError:
                continue
        elif "p10" in tag.get("class", []):
            if current_group:
                line_groups.append(current_group)
                current_group = []

    if current_group:
        line_groups.append(current_group)

    line_map = {}
    for line_id, group in enumerate(line_groups, start=1):
        for line_pos, car_number in enumerate(group, start=1):
            line_map[str(car_number)] = {
                "line_id": line_id,
                "line_pos": line_pos
            }

    return line_map

def extract_result_pairs(text):
    # æˆç¸¾æ¬„ã‹ã‚‰ã€Œåˆ3æº–1ã€â†’ ['åˆ3', 'æº–1'] ã®ã‚ˆã†ã«åˆ†å‰²
    return re.findall(r"[^\d\s]+\d+", text)[:4] + [""] * (4 - len(re.findall(r"[^\d\s]+\d+", text)))

def fetch_entry_data(url):
    print("âœ… fetch_entry_data() é–‹å§‹")
    print("ğŸ”— URL:", url)
    try:
        match = re.search(r"/athletes/(\d{4}-\d{2}-\d{2})/(\d+)/(\d+)", url)
        if not match:
            return {"error": "URLå½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“"}

        date_str, place_code, race_num = match.groups()

        options = Options()
        options.add_argument("--headless=new")
        options.add_argument("--ignore-certificate-errors")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")
        options.add_argument("--window-size=1280,800")
        driver = webdriver.Chrome(options=options)
        driver.get(url)

        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.XPATH, "//th[contains(text(), 'é¸æ‰‹å')]"))
            )
        except TimeoutException:
            html = driver.page_source
            save_path = os.path.abspath("entry_page_debug_on_error.html")
            with open(save_path, "w", encoding="utf-8") as f:
                f.write(html)
            print(f"âŒ WebDriverWaitã§å¤±æ•—ã€‚å–å¾—HTMLã‚’ä¿å­˜ã—ã¾ã—ãŸ: {save_path}")
            return {"error": "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼"}

        html = driver.page_source
        if not html:
            print("âŒ HTMLãŒç©ºã§ã™")
            return {"error": "HTMLãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ"}
        driver.quit()

        soup = BeautifulSoup(html, "html.parser")
        print("ğŸ§ª BeautifulSoup ã«ã‚ˆã‚‹ HTML ãƒ‘ãƒ¼ã‚¹å®Œäº†")
        line_map = extract_lineinfo_from_html(html)

        tables = soup.find_all("table")
        target_table = None
        for table in tables:
            if "é¸æ‰‹å" in table.get_text():
                target_table = table
                break

        if not target_table:
            return {"error": "é¸æ‰‹ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"}

        rows = target_table.select("tbody tr")
        print("ğŸ‘¥ æŠ½å‡ºã—ãŸè¡Œæ•°ï¼ˆé¸æ‰‹è¡Œï¼‰:", len(rows))
        data = []
        prev_frame = ""
        for row in rows:
            tds = row.find_all(["td", "th"])
            # æŸ”è»Ÿãªåˆ¤å®šã«å¤‰æ›´ï¼ˆcar_numberãŒå–ã‚Œã‚‹ã‹ã§åˆ¤æ–­ï¼‰
            if len(tds) < 20:
                continue

            if len(tds) >= 34:
                frame_number = tds[1].get_text(strip=True)
                tds_offset = 0
                prev_frame = frame_number
            else:
                frame_number = prev_frame
                tds_offset = -1

            try:
                car_number = tds[2 + tds_offset].get_text(strip=True)
                name = tds[3 + tds_offset].get_text(strip=True)
                age = tds[4 + tds_offset].get_text(strip=True)
                prefecture = tds[5 + tds_offset].get_text(strip=True)
                term = tds[6 + tds_offset].get_text(strip=True)
                grade = tds[7 + tds_offset].get_text(strip=True)
                leg_type = tds[8 + tds_offset].get_text(strip=True)
                gear = tds[9 + tds_offset].get_text(strip=True)
                score = tds[10 + tds_offset].get_text(strip=True)
                first_places = tds[11 + tds_offset].get_text(strip=True)
                second_places = tds[12 + tds_offset].get_text(strip=True)
                third_places = tds[13 + tds_offset].get_text(strip=True)
                outs = tds[14 + tds_offset].get_text(strip=True)
                win_rate = tds[15 + tds_offset].get_text(strip=True)
                top2_rate = tds[16 + tds_offset].get_text(strip=True)
                top3_rate = tds[17 + tds_offset].get_text(strip=True)
                style_escape = tds[18 + tds_offset].get_text(strip=True)
                style_sprint = tds[19 + tds_offset].get_text(strip=True)
                style_chase = tds[20 + tds_offset].get_text(strip=True)
                style_other = tds[21 + tds_offset].get_text(strip=True)
                start_number = tds[22 + tds_offset].get_text(strip=True)
                back_number = tds[23 + tds_offset].get_text(strip=True)
                recent_place1 = tds[24 + tds_offset].get_text(strip=True)
                recent_date1 = tds[25 + tds_offset].get_text(strip=True)
                recent_result1_raw = tds[26 + tds_offset].get_text(strip=True)
                recent_place2 = tds[27 + tds_offset].get_text(strip=True)
                recent_date2 = tds[28 + tds_offset].get_text(strip=True)
                recent_result2_raw = tds[29 + tds_offset].get_text(strip=True)
                recent_place3 = tds[30 + tds_offset].get_text(strip=True)
                recent_date3 = tds[31 + tds_offset].get_text(strip=True)
                recent_result3_raw = tds[32 + tds_offset].get_text(strip=True)
                comment = tds[33 + tds_offset].get_text(strip=True)
            except IndexError:
                continue

            if not car_number:
                continue
                continue

            line_info = line_map.get(car_number, {"line_id": None, "line_pos": None})

            r1 = extract_result_pairs(recent_result1_raw)
            r2 = extract_result_pairs(recent_result2_raw)
            r3 = extract_result_pairs(recent_result3_raw)

            data.append({
                "date": date_str,
                "place_code": place_code,
                "race_num": race_num,
                "frame_number": frame_number,
                "car_number": car_number,
                "name": name,
                "age": age,
                "prefecture": prefecture,
                "term": term,
                "grade": grade,
                "leg_type": leg_type,
                "gear": gear,
                "score": score,
                "first_places": first_places,
                "second_places": second_places,
                "third_places": third_places,
                "outs": outs,
                "win_rate": win_rate,
                "top2_rate": top2_rate,
                "top3_rate": top3_rate,
                "style_escape": style_escape,
                "style_sprint": style_sprint,
                "style_chase": style_chase,
                "style_other": style_other,
                "start_number": start_number,
                "back_number": back_number,
                "recent_place1": recent_place1,
                "recent_date1": recent_date1,
                **{f"recent_result1_{i+1}": r1[i] for i in range(4)},
                "recent_place2": recent_place2,
                "recent_date2": recent_date2,
                **{f"recent_result2_{i+1}": r2[i] for i in range(4)},
                "recent_place3": recent_place3,
                "recent_date3": recent_date3,
                **{f"recent_result3_{i+1}": r3[i] for i in range(4)},
                "comment": comment,
                "line_id": line_info["line_id"],
                "line_pos": line_info["line_pos"]
            })
        print("ğŸ“¦ ç™»éŒ²é¸æ‰‹æ•°:", len(data))

        return {
            "status": "OK",
            "entries": data,
            "meta": {
                "date": date_str,
                "place_code": place_code,
                "race_num": race_num
            }
        }
    except Exception as e:
        html = driver.page_source
        save_path = os.path.abspath("entry_page_debug_on_error.html")
        with open(save_path, "w", encoding="utf-8") as f:
            f.write(html)
        print(f"âŒ WebDriverWaitã§å¤±æ•—ã€‚å–å¾—HTMLã‚’ä¿å­˜ã—ã¾ã—ãŸ: {save_path}")
        return {"error": str(e)}
